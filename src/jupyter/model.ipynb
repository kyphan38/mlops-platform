{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c5d312-dd34-4871-9bc0-0c5b443f4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dict = {\n",
    "    \"id\": [2, 3],\n",
    "    \"host_id\": [2992450.0, 5651579.0],\n",
    "    \"accommodates\": [4.0, 2.0],\n",
    "    \"bathrooms\": [1.0, 1.0],\n",
    "    \"bedrooms\": [2.0, 1.0],\n",
    "    \"beds\": [2.2361068367009524, 0.0],\n",
    "    \"availability_30\": [0.0, 11.0],\n",
    "    \"availability_60\": [0.0, 15.0],\n",
    "    \"availability_90\": [0.0, np.nan],\n",
    "    \"availability_365\": [36.0, 15.0],\n",
    "    \"host_response_rate\": [100.0, 100.0],\n",
    "    \"host_acceptance_rate\": [100.0, 99.0],\n",
    "    \"host_listings_count\": [1.0, 2.0],\n",
    "    \"host_total_listings_count\": [5.0, 4.0],\n",
    "    \"number_of_reviews\": [9.0, 64.12143150995708],\n",
    "    \"number_of_reviews_ltm\": [0.0, 28.0],\n",
    "    \"number_of_reviews_l30d\": [0.0, 2.0],\n",
    "    \"review_scores_rating\": [np.nan, 4.51],\n",
    "    \"review_scores_accuracy\": [4.879706019274778, 4.61],\n",
    "    \"review_scores_cleanliness\": [4.812461133714737, 4.45],\n",
    "    \"review_scores_checkin\": [np.nan, 4.82],\n",
    "    \"review_scores_communication\": [4.944250346120556, 4.87],\n",
    "    \"review_scores_location\": [4.867251870622564, 4.79],\n",
    "    \"review_scores_value\": [4.790738114906661, 4.64],\n",
    "    \"reviews_per_month\": [0.08, 3.13],\n",
    "    \"minimum_nights\": [28.0, 1.0],\n",
    "    \"maximum_nights\": [1125.0, 45.0],\n",
    "    \"minimum_minimum_nights\": [28.0, np.nan],\n",
    "    \"maximum_minimum_nights\": [28.0, 2.0],\n",
    "    \"minimum_maximum_nights\": [1125.0, 1125.0],\n",
    "    \"maximum_maximum_nights\": [1125.0, 1125.0],\n",
    "    \"minimum_nights_avg_ntm\": [28.0, 2.0],\n",
    "    \"maximum_nights_avg_ntm\": [1125.0, 1125.0]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Ensure all columns are of type float\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9cb6b6-0de4-4f36-a291-47fb8dbf88a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/mlflow/utils/requirements_utils.py:20: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources  # noqa: TID251\n",
      "/opt/conda/lib/python3.11/site-packages/mlflow/gateway/config.py:61: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  @validator(\"togetherai_api_key\", pre=True)\n",
      "/opt/conda/lib/python3.11/site-packages/mlflow/gateway/config.py:390: PydanticDeprecatedSince20: Pydantic V1 style `@root_validator` validators are deprecated. You should migrate to Pydantic V2 style `@model_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  @root_validator(skip_on_failure=True)\n",
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_config.py:284: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from feast import FeatureStore\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema\n",
    "from mlflow.types.schema import ParamSchema\n",
    "from mlflow.types.schema import ParamSpec\n",
    "from mlflow.types.schema import ColSpec\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, RobustScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from utils.model.feature_vars import listing_features, host_features, review_features, fact_features\n",
    "from utils.model.feast import query_data, get_historical_features\n",
    "from utils.model.checking import df_description\n",
    "from utils.model.transforming import yeo_johnson_transforming\n",
    "from utils.model.imputation import missing_data_handling\n",
    "from utils.mlflow.creating import create_mlflow_experiment\n",
    "\n",
    "db_config = {\n",
    "  'user': 'admin',\n",
    "  'password': 'admin123',\n",
    "  'host': 'feast_postgres',\n",
    "  'port': '5432',\n",
    "  'database': 'feast_postgres'\n",
    "}\n",
    "\n",
    "def data_extraction():\n",
    "  fs = FeatureStore(repo_path=\"./feature_repo\")\n",
    "\n",
    "  connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "  engine = create_engine(connection_string)\n",
    "\n",
    "  queries = {\n",
    "    'listing': \"SELECT id, event_timestamp FROM listing_table\",\n",
    "    'host': \"SELECT host_id, event_timestamp FROM host_table\",\n",
    "    'review': \"SELECT id, event_timestamp FROM review_table\",\n",
    "    'fact': \"SELECT id, event_timestamp FROM fact_table\"\n",
    "  }\n",
    "\n",
    "  with ThreadPoolExecutor() as executor:\n",
    "    listing_data, host_data, review_data, fact_data = executor.map(lambda q: query_data(engine, q), queries.values())\n",
    "\n",
    "  listing_df = get_historical_features(fs, listing_data, listing_features)\n",
    "  host_df = get_historical_features(fs, host_data, host_features)\n",
    "  review_df = get_historical_features(fs, review_data, review_features)\n",
    "  fact_df = get_historical_features(fs, fact_data, fact_features)\n",
    "\n",
    "  # Drop event_timestamp columns\n",
    "  for df in [listing_df, host_df, review_df, fact_df]:\n",
    "    df.drop(columns=['event_timestamp'], inplace=True)\n",
    "\n",
    "  # Merge dataframes\n",
    "  df = pd.merge(listing_df, host_df, on=\"host_id\", how=\"left\").drop_duplicates(subset=['id'])\n",
    "  df = pd.merge(df, review_df, on=\"id\", how=\"left\").drop_duplicates(subset=['id'])\n",
    "  df = pd.merge(df, fact_df, on=\"id\", how=\"left\").drop_duplicates(subset=['id'])\n",
    "\n",
    "  print(f\"Length listing_df {len(listing_df)}\")\n",
    "  print(f\"Length host_df {len(host_df)}\")\n",
    "  print(f\"Length review_df {len(review_df)}\")\n",
    "  print(f\"Length fact_df {len(fact_df)}\")\n",
    "  print(f\"Length df {len(df)}\")\n",
    "\n",
    "  df.to_csv(\"./data.csv\", index=False)\n",
    "\n",
    "  return df\n",
    "\n",
    "def data_validation(df):\n",
    "  # report = {}\n",
    "\n",
    "  # missing_values_count = df.isnull().sum()\n",
    "  # vals = []\n",
    "\n",
    "  # print(f\"Length of dataframe: {len(df)}\\n\")\n",
    "\n",
    "  # for col in df.columns:\n",
    "  #   missing_count = missing_values_count[col]\n",
    "  #   col_type = df[col].dtype\n",
    "  #   vals.append(f\"Column: {col}, Missing Values: {missing_count}, Type: {col_type}\")\n",
    "\n",
    "  # vals = \"\\n\".join(vals)\n",
    "  # print(vals)\n",
    "\n",
    "  return df\n",
    "\n",
    "def data_preparation(df):\n",
    "  # Data dropping\n",
    "  df.drop(columns=[\"id\", \"host_id\"], axis=1, inplace=True)\n",
    "\n",
    "  # Data transforming\n",
    "  df = yeo_johnson_transforming(df)\n",
    "  \n",
    "  # Data splitting\n",
    "  features = df.drop(\"price\", axis=1)\n",
    "  target = df[\"price\"]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "  \n",
    "  # Data standardizing\n",
    "  scaler = RobustScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train)\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "  print(X_train_scaled.shape)\n",
    "\n",
    "  return X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def model_training(X_train_scaled, y_train):\n",
    "  models_and_params = {\n",
    "    \"Linear_Regression\": (LinearRegression(), {}),\n",
    "    \"Ridge_Regression\": (Ridge(), {\"alpha\": [0.001, 0.01, 0.1, 1, 10]}),\n",
    "    \"Lasso_Regression\": (Lasso(), {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1]}),\n",
    "    \"Bayesian_Ridge_Regression\": (BayesianRidge(), {\"alpha_1\": [1e-6, 1e-5, 1e-4], \"lambda_1\": [1e-6, 1e-5, 1e-4]}),\n",
    "    \"ElasticNet_Regression\": (ElasticNet(), {\"alpha\": [0.001, 0.01, 0.1], \"l1_ratio\": [0.1, 0.2, 0.5]}),\n",
    "    \"Decision_Tree_Regression\": (DecisionTreeRegressor(), {\"max_depth\": [5, 10, 20, None]})\n",
    "  }\n",
    "\n",
    "  trained_models = {}\n",
    "  for name, (model, param_grid) in models_and_params.items():\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = grid_search.best_estimator_\n",
    "\n",
    "  return trained_models\n",
    "\n",
    "def model_scoring(trained_models, X_test_scaled, y_test):\n",
    "  res = []\n",
    "  best_model = None\n",
    "  best_r2 = -float('inf')\n",
    "  \n",
    "  for name, model in trained_models.items():\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    res.append((name, model, model.get_params(), rmse, r2))\n",
    "\n",
    "    if r2 > best_r2:\n",
    "      best_r2 = r2\n",
    "      best_model_name = name\n",
    "      \n",
    "  return res, best_model_name\n",
    "\n",
    "def model_exporting(res, best_model_name, model_signature):\n",
    "  mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "  experiment_id = create_mlflow_experiment(\n",
    "    experiment_name=\"dev_model5\",\n",
    "    artifact_location=\"s3://artifacts\"\n",
    "    )\n",
    "  best_model = None\n",
    "  best_r2 = -float(\"inf\")\n",
    "  \n",
    "  with mlflow.start_run(run_name=\"experiment\", experiment_id=experiment_id) as run:\n",
    "    for name, model, params, rmse, r2 in res:\n",
    "      with mlflow.start_run(run_name=name, nested=True) as nested_run: \n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "        if name == best_model_name:\n",
    "          best_model_run_id = nested_run.info.run_id\n",
    "          mlflow.sklearn.log_model(model, artifact_path=name, signature=model_signature, registered_model_name=name)\n",
    "          print(f\"The best model is: {name}\")\n",
    "          print(f\"run_id of best_model: {best_model_run_id}\")\n",
    "              \n",
    "    print(f\"run_id of models: {run.info.run_id}\")\n",
    "\n",
    "  return best_model_run_id\n",
    "\n",
    "def model_signature(X_train):\n",
    "  cols_spec = []\n",
    "  data_map = {\n",
    "    \"int64\": \"integer\",\n",
    "    \"float64\": \"double\",\n",
    "    \"bool\": \"boolean\",\n",
    "    \"str\": \"string\",\n",
    "    \"object\": \"string\",\n",
    "    \"date\": \"datetime\",\n",
    "  }\n",
    "  for name, dtype in X_train.dtypes.to_dict().items():\n",
    "    cols_spec.append(ColSpec(name=name, type=data_map[str(dtype)]))\n",
    "    \n",
    "  input_schema = Schema(inputs=cols_spec)\n",
    "  output_schema = Schema([ColSpec(name=\"price\", type=\"double\")])\n",
    "  param = ParamSpec(name=\"model_name\", dtype=\"string\", default=\"model1\")\n",
    "  param_schema = ParamSchema(params=[param])\n",
    "\n",
    "  model_signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=param_schema)\n",
    "\n",
    "  return model_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a35610-a049-4568-8fca-b9c0fb6c6e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180888, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dev_model5 already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/05 04:54:43 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "/opt/conda/lib/python3.11/importlib/__init__.py:126: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "2024/06/05 04:55:50 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under s3://artifacts/7352439eb158417890265c912441d768/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n",
      "Registered model 'Decision_Tree_Regression' already exists. Creating a new version of this model...\n",
      "2024/06/05 04:55:50 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Decision_Tree_Regression, version 3\n",
      "Created version '3' of model 'Decision_Tree_Regression'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is: Decision_Tree_Regression\n",
      "run_id of best_model: 7352439eb158417890265c912441d768\n",
      "run_id of models: d37b6ac4ed2a4659b63486c5fd185b56\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "  # df = data_extraction()\n",
    "  df = pd.read_csv(\"./data.csv\")\n",
    "  df = data_validation(df)\n",
    "  X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test = data_preparation(df)\n",
    "  ms = model_signature(X_train)\n",
    "  trained_models = model_training(X_train_scaled, y_train)\n",
    "  res, best_model_name = model_scoring(trained_models, X_test_scaled, y_test)\n",
    "  best_model_run_id = model_exporting(res, best_model_name, ms)\n",
    "  # best_model_run_id, best_model_name = \"adc12572abe04f1eb83ab14a1fb0f111\", \"Decision_Tree_Regression\"\n",
    "\n",
    "  # sample_data = data.copy()\n",
    "  # model = load_best_model(best_model_run_id, best_model_name)\n",
    "  # model_serving = ModelServing(model)\n",
    "  # prediction = model_serving.data_prediction(sample_data)\n",
    "  # print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d9b494-5441-4f55-b6e9-9be7091d74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/mlflow/utils/requirements_utils.py:20: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources  # noqa: TID251\n",
      "/opt/conda/lib/python3.11/site-packages/mlflow/gateway/config.py:61: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  @validator(\"togetherai_api_key\", pre=True)\n",
      "/opt/conda/lib/python3.11/site-packages/mlflow/gateway/config.py:390: PydanticDeprecatedSince20: Pydantic V1 style `@root_validator` validators are deprecated. You should migrate to Pydantic V2 style `@model_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  @root_validator(skip_on_failure=True)\n",
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_config.py:284: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 9/9 [00:00<00:00, 678.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor()\n",
      "[IterativeImputer] Completing matrix with shape (2, 31)\n",
      "[IterativeImputer] Ending imputation round 1/5, elapsed time 0.03\n",
      "[IterativeImputer] Change: 0.0, scaled tolerance: 1.125 \n",
      "[IterativeImputer] Early stopping criterion reached.\n",
      "Prediction: [289. 103.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from utils.model.transforming import yeo_johnson_transforming\n",
    "from utils.model.imputation import missing_data_handling\n",
    "\n",
    "data_dict = {\n",
    "    \"id\": [2, 3],\n",
    "    \"host_id\": [2992450.0, 5651579.0],\n",
    "    \"accommodates\": [4.0, 2.0],\n",
    "    \"bathrooms\": [1.0, 1.0],\n",
    "    \"bedrooms\": [2.0, 1.0],\n",
    "    \"beds\": [2.2361068367009524, 0.0],\n",
    "    \"availability_30\": [0.0, 11.0],\n",
    "    \"availability_60\": [0.0, 15.0],\n",
    "    \"availability_90\": [0.0, np.nan],\n",
    "    \"availability_365\": [36.0, 15.0],\n",
    "    \"host_response_rate\": [100.0, 100.0],\n",
    "    \"host_acceptance_rate\": [100.0, 99.0],\n",
    "    \"host_listings_count\": [1.0, 2.0],\n",
    "    \"host_total_listings_count\": [5.0, 4.0],\n",
    "    \"number_of_reviews\": [9.0, 64.12143150995708],\n",
    "    \"number_of_reviews_ltm\": [0.0, 28.0],\n",
    "    \"number_of_reviews_l30d\": [0.0, 2.0],\n",
    "    \"review_scores_rating\": [np.nan, 4.51],\n",
    "    \"review_scores_accuracy\": [4.879706019274778, 4.61],\n",
    "    \"review_scores_cleanliness\": [4.812461133714737, 4.45],\n",
    "    \"review_scores_checkin\": [np.nan, 4.82],\n",
    "    \"review_scores_communication\": [4.944250346120556, 4.87],\n",
    "    \"review_scores_location\": [4.867251870622564, 4.79],\n",
    "    \"review_scores_value\": [4.790738114906661, 4.64],\n",
    "    \"reviews_per_month\": [0.08, 3.13],\n",
    "    \"minimum_nights\": [28.0, 1.0],\n",
    "    \"maximum_nights\": [1125.0, 45.0],\n",
    "    \"minimum_minimum_nights\": [28.0, np.nan],\n",
    "    \"maximum_minimum_nights\": [28.0, 2.0],\n",
    "    \"minimum_maximum_nights\": [1125.0, 1125.0],\n",
    "    \"maximum_maximum_nights\": [1125.0, 1125.0],\n",
    "    \"minimum_nights_avg_ntm\": [28.0, 2.0],\n",
    "    \"maximum_nights_avg_ntm\": [1125.0, 1125.0]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Ensure all columns are of type float\n",
    "data = data.astype(float)\n",
    "\n",
    "from feast import FeatureStore\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "def load_best_model(run_id, model_name):\n",
    "  model_uri = f\"runs:/{run_id}/{model_name}\"\n",
    "  model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "  return model\n",
    "\n",
    "class ModelServing:\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    \n",
    "  def data_transforming(self, df):\n",
    "    # Data dropping\n",
    "    df.drop(columns=[\"id\", \"host_id\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Outliers handling\n",
    "    df = missing_data_handling(df)\n",
    "\n",
    "    # Data transforming\n",
    "    df = yeo_johnson_transforming(df)\n",
    "    \n",
    "    # Data standardizing\n",
    "    scaler = RobustScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "  def data_prediction(self, sample_data):\n",
    "    data = self.data_transforming(sample_data)\n",
    "    pred = self.model.predict(data)\n",
    "\n",
    "    return pred\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "  best_model_run_id, best_model_name = \"6f5ea1d794804da681cc85c0ff02c9dc\", \"Decision_Tree_Regression\"\n",
    "  model = load_best_model(best_model_run_id, best_model_name)\n",
    "  print(model)\n",
    "  sample_data = data.copy()\n",
    "  model_serving = ModelServing(model)\n",
    "  prediction = model_serving.data_prediction(sample_data)\n",
    "  print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1085c-1db7-48a2-b355-824edd6d0c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
