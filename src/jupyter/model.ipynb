{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9cb6b6-0de4-4f36-a291-47fb8dbf88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from feast import FeatureStore\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, RobustScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from utils.model.feature_vars import listing_features, host_features, review_features, fact_features\n",
    "from utils.model.feast import query_data, get_historical_features\n",
    "from utils.model.checking import df_description\n",
    "from utils.model.outliers import outliers_handling\n",
    "from utils.model.encoding import data_encoding\n",
    "from utils.mlflow.creating import create_mlflow_experiment\n",
    "\n",
    "db_config = {\n",
    "  'user': 'admin',\n",
    "  'password': 'admin123',\n",
    "  'host': 'feast_postgres',\n",
    "  'port': '5432',\n",
    "  'database': 'feast_postgres'\n",
    "}\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "\n",
    "def data_extraction():\n",
    "  fs = FeatureStore(repo_path=\"./feature_repo\")\n",
    "\n",
    "  connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "  engine = create_engine(connection_string)\n",
    "\n",
    "  queries = {\n",
    "    'listing': \"SELECT id, event_timestamp FROM listing_table\",\n",
    "    'host': \"SELECT host_id, event_timestamp FROM host_table\",\n",
    "    'review': \"SELECT id, event_timestamp FROM review_table\",\n",
    "    'fact': \"SELECT id, event_timestamp FROM fact_table\"\n",
    "  }\n",
    "\n",
    "  with ThreadPoolExecutor() as executor:\n",
    "    listing_data, host_data, review_data, fact_data = executor.map(lambda q: query_data(engine, q), queries.values())\n",
    "\n",
    "  listing_df = get_historical_features(fs, listing_data, listing_features)\n",
    "  host_df = get_historical_features(fs, host_data, host_features)\n",
    "  review_df = get_historical_features(fs, review_data, review_features)\n",
    "  fact_df = get_historical_features(fs, fact_data, fact_features)\n",
    "\n",
    "  # Drop event_timestamp columns\n",
    "  for df in [listing_df, host_df, review_df, fact_df]:\n",
    "    df.drop(columns=['event_timestamp'], inplace=True)\n",
    "\n",
    "  # Merge dataframes\n",
    "  df = pd.merge(listing_df, host_df, on=\"host_id\", how=\"left\").drop_duplicates(subset=['id'])\n",
    "  df = pd.merge(df, review_df, on=\"id\", how=\"left\").drop_duplicates(subset=['id'])\n",
    "  df = pd.merge(df, fact_df, on=\"id\", how=\"left\").drop_duplicates(subset=['id'])\n",
    "\n",
    "  print(f\"Length listing_df {len(listing_df)}\")\n",
    "  print(f\"Length host_df {len(host_df)}\")\n",
    "  print(f\"Length review_df {len(review_df)}\")\n",
    "  print(f\"Length fact_df {len(fact_df)}\")\n",
    "  print(f\"Length df {len(df)}\")\n",
    "\n",
    "  return df\n",
    "\n",
    "def data_validation(df):\n",
    "  report = {}\n",
    "\n",
    "  missing_values_count = df.isnull().sum()\n",
    "  vals = []\n",
    "\n",
    "  print(f\"Length of dataframe: {len(df)}\\n\")\n",
    "\n",
    "  for col in df.columns:\n",
    "    missing_count = missing_values_count[col]\n",
    "    col_type = df[col].dtype\n",
    "    vals.append(f\"Column: {col}, Missing Values: {missing_count}, Type: {col_type}\")\n",
    "\n",
    "  vals = \"\\n\".join(vals)\n",
    "  print(vals)\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def data_preparation(df):\n",
    "  # Outliers handling\n",
    "  df = outliers_handling(df)\n",
    "\n",
    "  # Data encoding\n",
    "  df = data_encoding(df)\n",
    "\n",
    "  # Data splitting\n",
    "  features = df.drop(\"price\", axis=1)\n",
    "  target = df[\"price\"]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "  # Data standardizing\n",
    "  scaler = RobustScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train)\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def model_training(X_train_scaled, y_train):\n",
    "  models_and_params = {\n",
    "    \"Linear_Regression\": LinearRegression(),\n",
    "    \"Ridge_Regression\": Ridge(alpha=0.001),\n",
    "    \"Lasso_Regression\": Lasso(alpha=0.0001),\n",
    "    \"Bayesian_Ridge Regression\": BayesianRidge(alpha_1=1e-6, lambda_1=1e-6),\n",
    "    \"ElasticNet_Regression\": ElasticNet(alpha=0.01, l1_ratio=0.2),\n",
    "    \"Decision_Tree_Regression\": DecisionTreeRegressor(max_depth=3),\n",
    "  }\n",
    "\n",
    "  trained_models = {}\n",
    "  for name, model in models_and_params.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = model\n",
    "\n",
    "  return trained_models\n",
    "\n",
    "def model_validation(trained_models, X_test_scaled, y_test):\n",
    "  res = []\n",
    "  for name, model in trained_models.items():\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    res.append((name, model, model.get_params(), rmse, r2))\n",
    "\n",
    "  print(res)\n",
    "\n",
    "  return res\n",
    "\n",
    "def model_exporting(res):\n",
    "    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "    experiment_id = create_mlflow_experiment(\n",
    "      experiment_name=\"dev_model\",\n",
    "        # artifact_location=\"/home/jovyan/models\"  # Absolute path to the models folder\n",
    "      artifact_location=\"/mlflow/artifacts\"\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=\"experiment\", experiment_id=experiment_id):\n",
    "        for name, model, params, rmse, r2 in res:\n",
    "            sanitized_name = name.replace(\" \", \"_\")\n",
    "            with mlflow.start_run(run_name=name, nested=True) as run: \n",
    "                mlflow.log_params(params)\n",
    "                mlflow.log_metric(\"rmse\", rmse)\n",
    "                mlflow.log_metric(\"r2\", r2)\n",
    "                mlflow.sklearn.log_model(model, artifact_path=name)\n",
    "\n",
    "      \n",
    "import joblib\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  df = data_extraction()\n",
    "  df = data_validation(df)\n",
    "  X_train_scaled, X_test_scaled, y_train, y_test = data_preparation(df)\n",
    "  trained_models = model_training(X_train_scaled, y_train)\n",
    "  res = model_validation(trained_models, X_test_scaled, y_test)\n",
    "  model_exporting(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c382fb6-8e16-464c-a360-de4d7c1c6b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
